services:
  spark-master:
    image: bitnami/spark:3.5.2
    container_name: spark-master
    ports:
      - "7077:7077"
      - "8080:8080"
    env_file:
      - master.env
    networks:
      - spark-network

  spark-worker:
    image: bitnami/spark:3.5.2
    container_name: spark-worker
    ports:
      - "8081:8081" 
    env_file:
      - worker.env
    depends_on:
      - spark-master
    networks:
      - spark-network
  fastapi:
    build:
      context: ../../
      dockerfile: ./docker/fastapi/Dockerfile
    container_name: fastapi
    ports:
      - "8000:8000"
      - "4040:4040"
    depends_on:
      - spark-master
      - spark-worker
    volumes:
      - ../../app:/app  # Mount the local app directory to the container
      - ../../requirements.txt:/tmp/requirements.txt
      - ../../.env:/app/.env
    networks:
      - spark-network
  hadoop-namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.1.3-java8
    container_name: hadoop-namenode
    restart: always
    environment:
      - CLUSTER_NAME=cyberfuse-hadoop-cluster
    volumes:
      - hadoop_namenode:/hadoop/dfs/name
    ports:
      - "9870:9870"
      - "8020:8020"
    networks:
      - spark-network

  hadoop-datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.1.3-java8
    container_name: hadoop-datanode
    ports:
      - "9864:9864"
    environment:
      - CLUSTER_NAME=cyberfuse-hadoop-cluster
      - CORE_CONF_fs_defaultFS=hdfs://hadoop-namenode:8020
      - HDFS_CONF_dfs_datanode_data_dir_list=/hadoop/dfs/data

    volumes:
      - hadoop_datanode:/hadoop/dfs/data
      - ../../shared:/shared
    networks:
      - spark-network
    depends_on:
      - hadoop-namenode

networks:
  spark-network:
    driver: bridge

volumes:
  hadoop_namenode:
  hadoop_datanode: